{"cells":[{"cell_type":"markdown","metadata":{"id":"EnjTa1m1d1nq"},"source":["# Post Hoc Explaination on Adult"]},{"cell_type":"markdown","metadata":{"id":"LIZBm9jId900"},"source":["# Setup"]},{"cell_type":"markdown","metadata":{"id":"mZcDgaMbeBai"},"source":["In the following cells we are going to configure our notebook. First, we clone the xai library (Naretto, Bodria) and install the necessary packages with pip. Then we import the libraries that we need"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":37156,"status":"ok","timestamp":1670916597243,"user":{"displayName":"Roberto PELLUNGRINI","userId":"06244953192047321351"},"user_tz":-60},"id":"DWHr-7HmK724","outputId":"582479a2-9bbf-48ba-d619-f07f55b1744d"},"outputs":[{"name":"stderr","output_type":"stream","text":["Cloning into 'XAI_course_2024'...\n","error: RPC failed; curl 92 HTTP/2 stream 5 was not closed cleanly before end of the underlying stream\n","error: 4870 bytes of body are still expected\n","fetch-pack: unexpected disconnect while reading sideband packet\n","fatal: early EOF\n","fatal: fetch-pack: invalid index-pack output\n"]}],"source":["!git clone https://github.com/pellungrobe/XAI_course_2024"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":222,"status":"ok","timestamp":1670916643302,"user":{"displayName":"Roberto PELLUNGRINI","userId":"06244953192047321351"},"user_tz":-60},"id":"wu-qq51gdD4q","outputId":"5b872af2-9f31-4bfa-a679-cb90830a196d"},"outputs":[{"name":"stdout","output_type":"stream","text":["/Users/roberto/Documents/Insegnamento/XAI/2024/XAI_course_2024\n"]}],"source":["#cd XAI_course_2024"]},{"cell_type":"markdown","metadata":{},"source":["Remember to use the .yml file with the configuration:  conda config --set channel_priority flexible"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"EPk0MZMlJ30h"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'pandas'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomForestClassifier\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n","\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"]}],"source":["import numpy as np\n","import pandas as pd\n","\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","from sklearn import metrics\n","\n","import lime\n","import shap\n","import xgboost as xgb\n","\n","from xailib.data_loaders.dataframe_loader import prepare_dataframe\n","\n","from xailib.explainers.lime_explainer import LimeXAITabularExplainer\n","from xailib.explainers.lore_explainer import LoreTabularExplainer\n","from xailib.explainers.shap_explainer_tab import ShapXAITabularExplainer\n","\n","from xailib.models.sklearn_classifier_wrapper import sklearn_classifier_wrapper"]},{"cell_type":"markdown","metadata":{"id":"_S0G98zIM8jw"},"source":["# Input Data Preparation\n"]},{"cell_type":"markdown","metadata":{"id":"piS9oKGLhmTq"},"source":["We start by reading our data. \n","\n","We are going to work on the adult dataset. It is a classic, benchmark tabular dataset, designed for a classification task.\n","\n","The Adult dataset classifies people as high ($\\geq$ 50k) or low ($\\leq$50k) income.\n","\n","We read our data from a .csv file into a `pandas` `DataFrame`."]},{"cell_type":"code","execution_count":7,"metadata":{"id":"VyneWzJzKnoZ"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>age</th>\n","      <th>workclass</th>\n","      <th>fnlwgt</th>\n","      <th>education-num</th>\n","      <th>marital-status</th>\n","      <th>occupation</th>\n","      <th>relationship</th>\n","      <th>race</th>\n","      <th>sex</th>\n","      <th>capital-gain</th>\n","      <th>capital-loss</th>\n","      <th>hours-per-week</th>\n","      <th>native-country</th>\n","      <th>class</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>39</td>\n","      <td>0</td>\n","      <td>77516</td>\n","      <td>13</td>\n","      <td>7</td>\n","      <td>14</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>2174</td>\n","      <td>0</td>\n","      <td>40</td>\n","      <td>41</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>50</td>\n","      <td>1</td>\n","      <td>83311</td>\n","      <td>13</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>13</td>\n","      <td>41</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>38</td>\n","      <td>2</td>\n","      <td>215646</td>\n","      <td>9</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>40</td>\n","      <td>41</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>53</td>\n","      <td>2</td>\n","      <td>234721</td>\n","      <td>7</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>40</td>\n","      <td>41</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>28</td>\n","      <td>2</td>\n","      <td>338409</td>\n","      <td>13</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>40</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   age  workclass  fnlwgt  education-num  marital-status  occupation  \\\n","0   39          0   77516             13               7          14   \n","1   50          1   83311             13               1           1   \n","2   38          2  215646              9               2           2   \n","3   53          2  234721              7               1           2   \n","4   28          2  338409             13               1           3   \n","\n","   relationship  race  sex  capital-gain  capital-loss  hours-per-week  \\\n","0             5     1    0          2174             0              40   \n","1             1     1    0             0             0              13   \n","2             5     1    0             0             0              40   \n","3             1     2    0             0             0              40   \n","4             2     2    1             0             0              40   \n","\n","   native-country  class  \n","0              41      0  \n","1              41      0  \n","2              41      0  \n","3              41      0  \n","4               1      0  "]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["filename = 'tabular datasets/adult_clean.csv'\n","adult = pd.read_csv(filename, skipinitialspace=True, na_values='?', keep_default_na=True)\n","adult.head()"]},{"cell_type":"markdown","metadata":{"id":"5jzXx_PdfXja"},"source":["`class` is our target variable, the one that we want to predict"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"_Tqoy0zpgA2Y"},"outputs":[],"source":["target = 'class'"]},{"cell_type":"markdown","metadata":{"id":"W43HMRiSgOZ0"},"source":["We can look at some summary to have an idea of how our data is distributed"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"OPgNvrs4gK1L"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>age</th>\n","      <th>workclass</th>\n","      <th>fnlwgt</th>\n","      <th>education-num</th>\n","      <th>marital-status</th>\n","      <th>occupation</th>\n","      <th>relationship</th>\n","      <th>race</th>\n","      <th>sex</th>\n","      <th>capital-gain</th>\n","      <th>capital-loss</th>\n","      <th>hours-per-week</th>\n","      <th>native-country</th>\n","      <th>class</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>30162.000000</td>\n","      <td>30162.000000</td>\n","      <td>3.016200e+04</td>\n","      <td>30162.000000</td>\n","      <td>30162.000000</td>\n","      <td>30162.000000</td>\n","      <td>30162.000000</td>\n","      <td>30162.000000</td>\n","      <td>30162.000000</td>\n","      <td>30162.000000</td>\n","      <td>30162.000000</td>\n","      <td>30162.000000</td>\n","      <td>30162.000000</td>\n","      <td>30162.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>38.437902</td>\n","      <td>2.109343</td>\n","      <td>1.897938e+05</td>\n","      <td>10.121312</td>\n","      <td>3.332272</td>\n","      <td>6.342749</td>\n","      <td>2.775247</td>\n","      <td>1.211823</td>\n","      <td>0.324315</td>\n","      <td>1092.007858</td>\n","      <td>88.372489</td>\n","      <td>40.931238</td>\n","      <td>38.570420</td>\n","      <td>0.248922</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>13.134665</td>\n","      <td>0.934785</td>\n","      <td>1.056530e+05</td>\n","      <td>2.549995</td>\n","      <td>2.708290</td>\n","      <td>4.127163</td>\n","      <td>1.676177</td>\n","      <td>0.612461</td>\n","      <td>0.468126</td>\n","      <td>7406.346497</td>\n","      <td>404.298370</td>\n","      <td>11.979984</td>\n","      <td>8.429816</td>\n","      <td>0.432396</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>17.000000</td>\n","      <td>0.000000</td>\n","      <td>1.376900e+04</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>28.000000</td>\n","      <td>2.000000</td>\n","      <td>1.176272e+05</td>\n","      <td>9.000000</td>\n","      <td>1.000000</td>\n","      <td>3.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>40.000000</td>\n","      <td>41.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>37.000000</td>\n","      <td>2.000000</td>\n","      <td>1.784250e+05</td>\n","      <td>10.000000</td>\n","      <td>2.000000</td>\n","      <td>5.000000</td>\n","      <td>3.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>40.000000</td>\n","      <td>41.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>47.000000</td>\n","      <td>2.000000</td>\n","      <td>2.376285e+05</td>\n","      <td>13.000000</td>\n","      <td>7.000000</td>\n","      <td>10.000000</td>\n","      <td>5.000000</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>45.000000</td>\n","      <td>41.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>90.000000</td>\n","      <td>6.000000</td>\n","      <td>1.484705e+06</td>\n","      <td>16.000000</td>\n","      <td>7.000000</td>\n","      <td>14.000000</td>\n","      <td>5.000000</td>\n","      <td>5.000000</td>\n","      <td>1.000000</td>\n","      <td>99999.000000</td>\n","      <td>4356.000000</td>\n","      <td>99.000000</td>\n","      <td>41.000000</td>\n","      <td>1.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                age     workclass        fnlwgt  education-num  \\\n","count  30162.000000  30162.000000  3.016200e+04   30162.000000   \n","mean      38.437902      2.109343  1.897938e+05      10.121312   \n","std       13.134665      0.934785  1.056530e+05       2.549995   \n","min       17.000000      0.000000  1.376900e+04       1.000000   \n","25%       28.000000      2.000000  1.176272e+05       9.000000   \n","50%       37.000000      2.000000  1.784250e+05      10.000000   \n","75%       47.000000      2.000000  2.376285e+05      13.000000   \n","max       90.000000      6.000000  1.484705e+06      16.000000   \n","\n","       marital-status    occupation  relationship          race           sex  \\\n","count    30162.000000  30162.000000  30162.000000  30162.000000  30162.000000   \n","mean         3.332272      6.342749      2.775247      1.211823      0.324315   \n","std          2.708290      4.127163      1.676177      0.612461      0.468126   \n","min          1.000000      1.000000      1.000000      1.000000      0.000000   \n","25%          1.000000      3.000000      1.000000      1.000000      0.000000   \n","50%          2.000000      5.000000      3.000000      1.000000      0.000000   \n","75%          7.000000     10.000000      5.000000      1.000000      1.000000   \n","max          7.000000     14.000000      5.000000      5.000000      1.000000   \n","\n","       capital-gain  capital-loss  hours-per-week  native-country  \\\n","count  30162.000000  30162.000000    30162.000000    30162.000000   \n","mean    1092.007858     88.372489       40.931238       38.570420   \n","std     7406.346497    404.298370       11.979984        8.429816   \n","min        0.000000      0.000000        1.000000        1.000000   \n","25%        0.000000      0.000000       40.000000       41.000000   \n","50%        0.000000      0.000000       40.000000       41.000000   \n","75%        0.000000      0.000000       45.000000       41.000000   \n","max    99999.000000   4356.000000       99.000000       41.000000   \n","\n","              class  \n","count  30162.000000  \n","mean       0.248922  \n","std        0.432396  \n","min        0.000000  \n","25%        0.000000  \n","50%        0.000000  \n","75%        0.000000  \n","max        1.000000  "]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["adult.describe()"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"A8KNzdCwgLJe"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 30162 entries, 0 to 30161\n","Data columns (total 14 columns):\n"," #   Column          Non-Null Count  Dtype\n","---  ------          --------------  -----\n"," 0   age             30162 non-null  int64\n"," 1   workclass       30162 non-null  int64\n"," 2   fnlwgt          30162 non-null  int64\n"," 3   education-num   30162 non-null  int64\n"," 4   marital-status  30162 non-null  int64\n"," 5   occupation      30162 non-null  int64\n"," 6   relationship    30162 non-null  int64\n"," 7   race            30162 non-null  int64\n"," 8   sex             30162 non-null  int64\n"," 9   capital-gain    30162 non-null  int64\n"," 10  capital-loss    30162 non-null  int64\n"," 11  hours-per-week  30162 non-null  int64\n"," 12  native-country  30162 non-null  int64\n"," 13  class           30162 non-null  int64\n","dtypes: int64(14)\n","memory usage: 3.2 MB\n"]}],"source":["adult.info()"]},{"cell_type":"markdown","metadata":{"id":"oVihZMiSggO4"},"source":["After the data is loaded in memory, we need to extract metadata information to automatically handle the content withint the table.\n","\n","We can do this on our own, but the loader method `prepare_dataframe` can make a lot of work for us.\n","The method `prepare_dataframe` scans the table and extract the following information:\n","\n","- `df`: is a trasformed version of the original dataframe, where discrete attributes are transformed into numerical attributes by using one hot encoding strategy;\n","- `feature_names`: is a list containint the names of the features after the transformation;\n","- `class_values`: the list of all the possible values for the class_field column;\n","- `numeric_columns`: a list of the original features that contain numeric (i.e. continuous) values;\n","- `rdf`: the original dataframe, before the transformation;\n","- `real_feature_names`: the list of the features of the dataframe before the transformation;\n","- `features_map`: it is a dictionary pointing each feature to the original one before the transformation."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g1QujbHphMDj"},"outputs":[],"source":["df, feature_names, class_values, numeric_columns, rdf, real_feature_names, features_map = prepare_dataframe(adult, target)"]},{"cell_type":"markdown","metadata":{"id":"Y7hD5RnChWfs"},"source":["# Let's train a black box on our data"]},{"cell_type":"markdown","metadata":{"id":"kD_dsPxbhobI"},"source":["Now that the data is ready we can train a classifier on it. Let's try the classic `random forest classifier`.\n"]},{"cell_type":"markdown","metadata":{"id":"aM-RM65Uh2fd"},"source":["First, we split train and test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Li-QlNI4h2wI"},"outputs":[],"source":["test_size = 0.3\n","random_state = 42\n","X_train, X_test, Y_train, Y_test = train_test_split(df[feature_names], df[target],\n","                                                        test_size=test_size,\n","                                                        random_state=random_state,\n","                                                        stratify=df[target])"]},{"cell_type":"markdown","metadata":{"id":"m2nLIrhDiXQA"},"source":["We can then create our black box and train it. \n","We should do some hyper parameter search first, possibly with cross validation, but this is just an example"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g-2DHnoQiXZG"},"outputs":[],"source":["bb = RandomForestClassifier(n_estimators=20, random_state=random_state)\n","bb.fit(X_train.values, Y_train.values)\n"," "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0K0R9KTUi19l"},"outputs":[],"source":["Y_pred = bb.predict(X_train)\n","print(classification_report(Y_train, Y_pred))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fO4E0MmEi5iy"},"outputs":[],"source":["Y_pred = bb.predict(X_test)\n","print(classification_report(Y_test, Y_pred))"]},{"cell_type":"markdown","metadata":{"id":"itlVa9czjBMy"},"source":["Obviously, train and test present very different metrics. \n","Now we can apply a post-hoc explanation method to our black box. "]},{"cell_type":"markdown","metadata":{},"source":["I will also train an alternative model, an xgboost classifier. The reason will become clear shortly."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["xgbclf = xgb.XGBClassifier().fit(X_train, Y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sklearn import metrics\n","\n","import lime\n","import shap\n","import xgboost"]},{"cell_type":"markdown","metadata":{"id":"sXA6GHvdwygW"},"source":["# SHAP\n"]},{"cell_type":"markdown","metadata":{"id":"0nxbxOZiw1Eb"},"source":["SHAP uses game theory and shapley values to provide an explanation.\n","\n","The general working of SHAP in the classical Kernel mode (default mode) is:\n","\n","- Sample coalitions $z_k'\\in\\{0,1\\}^M,\\quad{}k\\in\\{1,\\ldots,K\\}$ (1 = feature present in coalition, 0 = feature absent).\n","- Get prediction for each $z_k'$ by first converting $z_k'$ to the original feature space and then applying model $\\hat{f}: \\hat{f}(h_x(z_k'))$\n","- Compute the weight for each $z_k'$ with the SHAP kernel.\n","- Fit weighted linear model.\n","- Return Shapley values $\\phi_k$, the coefficients from the linear model."]},{"cell_type":"markdown","metadata":{"id":"NLnRqZjByM7K"},"source":["SHAP provides several explanators, optimized for different objectives, and several visualization options.\n","\n","- Kernel Explainer, it is the *real* agnostic method for computing Shap values. Hence, it works for every kind of ML model (quite slow).\n","- Linear Explainer, for explaining linear models. It is an exact method. It allows for an analysis with independent variables, as well as for correlated ones (you have to estimates the variables covariance matrix).\n","- Tree Explainer, for explaining tree-based models (tree and ensambles). Is is an exact algorithm. (Difference: conditional expectation $E_{X_j|X_{-j}}(\\hat{f}(x)|x_j)$)\n","- Gradient Explainer, for explaining deep learning models. It is based on Integrated Gradient and it is an approximation algorithm.\n","- Deep Explainer, for explaining deep learning models. It is based on DEEPLift and the method computes an approximation algorithm (faster than Gradient Explainer)."]},{"cell_type":"markdown","metadata":{"id":"8g3TclI9ylcb"},"source":["For our random forest we can use the tree explainer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kf2FWlgeyhCX"},"outputs":[],"source":["shap_tree_exp = shap.TreeExplainer(bb)\n","shap_values_tree = shap_tree_exp.shap_values(X_test[0:200])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["shap_values_tree"]},{"cell_type":"markdown","metadata":{"id":"UT8NVjjgyzsJ"},"source":["The summary plot gives us an overall view of the shapley values for a bunch of records"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5ZG8DBeP1-vO"},"outputs":[],"source":["shap.summary_plot(shap_values_tree, X_test[0:200])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tree_explanation_object = shap_tree_exp(X_test[0:200])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tree_explanation_object"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["shap.plots.bar(tree_explanation_object[:,:,0])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["shap.summary_plot(shap_values_tree[0], X_test[0:200], plot_type='bar')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["xgb_shap_tree_exp = shap.TreeExplainer(xgbclf)\n","xgb_shap_values_tree = xgb_shap_tree_exp.shap_values(X_test[0:200])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["xgb_shap_values_tree"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["shap.summary_plot(xgb_shap_values_tree, X_test[0:200])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["shap.summary_plot(shap_values_tree[0], X_test[0:200])"]},{"cell_type":"markdown","metadata":{"id":"B9-dtSZF2MDW"},"source":["We can use dependence plot to focus on one variable.\n","\n","- Each dot is a single prediction (row) from the dataset.\n","- The x-axis is the value of the feature (from the X matrix).\n","- The y-axis is the SHAP value for that feature, which represents how much knowing that feature’s value changes the output of the model for that sample’s prediction.\n","- The color corresponds to a second feature that may have an interaction effect with the feature we are plotting (by default this second feature is chosen automatically). If an interaction effect is present between this other feature and the feature we are plotting it will show up as a distinct vertical pattern of coloring."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mlblIYjw2UPF"},"outputs":[],"source":["shap.dependence_plot(\"age\", shap_values_tree[0], X_test[0:200])"]},{"cell_type":"markdown","metadata":{"id":"qWVo1KWW2b_I"},"source":["We can explore the interactions between variables.\n","SHAP interaction values are a generalization of SHAP values to higher order interactions.\n","A summary plot of a SHAP interaction value matrix plots a matrix of summary plots with the main effects on the diagonal and the interaction effects off the diagonal."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bYyC1ywe2ggf"},"outputs":[],"source":["shap_interaction_values_tree = shap_tree_exp.shap_interaction_values(X_test.iloc[0:20])\n","shap.summary_plot(shap_interaction_values_tree[1], X_test.iloc[0:20])"]},{"cell_type":"markdown","metadata":{},"source":["Running a dependence plot on the SHAP interaction values a allows us to separately observe the main effects and the interaction effects."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IByoLfI42pjX"},"outputs":[],"source":["shap.dependence_plot(\n","    (\"age\", \"workclass\"),\n","    shap_interaction_values_tree[0], X_test.iloc[0:20]\n",")\n"]},{"cell_type":"markdown","metadata":{"id":"FJcSEAeQ2wCi"},"source":["SHAP also provides some nice interactive visualizations using javascript"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TEAwaRHl2uSB"},"outputs":[],"source":["shap.initjs()\n","shap.force_plot(shap_tree_exp.expected_value[0], shap_values_tree[0], X_test[0:200])"]},{"cell_type":"markdown","metadata":{"id":"glB6y1G926QT"},"source":["For now we've explored our model on a global level. Let's now look at local explanation using SHAP."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JWEzZ0Ag3AwR"},"outputs":[],"source":["shap.initjs()\n","shap.force_plot(shap_tree_exp.expected_value[0], shap_values_tree[0][0,:], feature_names)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["shap.plots.waterfall(tree_explanation_object[:,:,0][0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ASKpkLk96E0Z"},"outputs":[],"source":["shap.plots._waterfall.waterfall_legacy(shap_tree_exp.expected_value[0], shap_values_tree[0][0,:], feature_names=feature_names)"]},{"cell_type":"markdown","metadata":{"id":"lgUdt6Mw3R8z"},"source":["Another example\n","\n","Let's try a different explainer: before we used the TreeExplainer, but there are other explainers available in SHAP. Since we are dealing with a RandomForest, another possibility could be the KernelExplainer, that is completley agnostic.\n","\n","KernelExplainer requires in input the predict_proba of the black-box we want to explain and a sample of the data. In this simple example we use just the first 20 records of the test set, but in principle you can pass whatever representation of your data, such as the centroids obtained from a clustering algorithm."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oUFkBXqX3TzG"},"outputs":[],"source":["shap_kernel_exp = shap.KernelExplainer(bb.predict_proba, X_test[0:20])\n","shap_values_kernel = shap_kernel_exp.shap_values(X_test[0:200])"]},{"cell_type":"markdown","metadata":{"id":"0dHGRvfzAL-C"},"source":["Let's now see how the explanation change"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q_n3xyLW3Yjv"},"outputs":[],"source":["shap.initjs()\n","shap.force_plot(shap_kernel_exp.expected_value[0], shap_values_kernel[0], X_test[0:200])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2ZnaeJEz_9F8"},"outputs":[],"source":["shap.summary_plot(shap_values_kernel, X_test[0:200])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1V1VFQTKACux"},"outputs":[],"source":["shap.initjs()\n","shap.force_plot(shap_kernel_exp.expected_value[1], shap_values_kernel[1][1,:], feature_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T0yHIpyhoarX"},"outputs":[],"source":["shap.plots._waterfall.waterfall_legacy(shap_kernel_exp.expected_value[1], shap_values_kernel[1][1,:])"]},{"cell_type":"markdown","metadata":{"id":"nVr_HrArqLx3"},"source":["# LIME"]},{"cell_type":"markdown","metadata":{"id":"1D9Vej0TqP92"},"source":["LIME focuses on local explanations. The idea is: it generates a set of neighbours, on them it trains a linear model. \n","\n","- Select your instance of interest for which you want to have an explanation of its black box prediction.\n","- Perturb your dataset and  get the black box predictions for these new points.\n","- Weight the new samples according to their proximity to the instance of interest.\n","- Train a weighted, interpretable model on the dataset with the variations.\n","- Explain the prediction by interpreting the local model.\n","\n","\n","There are 2 kinds of neighbourhood generation:\n","\n","- Gaussian\n","- Lhs (Latin Hypercube Sampling)"]},{"cell_type":"markdown","metadata":{"id":"-1kbk-aFr0YQ"},"source":["We first initialize the lime explainer on our train data, providing the names of our features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YjRpnuwkqLwm"},"outputs":[],"source":["explainer = lime.lime_tabular.LimeTabularExplainer(X_train.values, feature_names = feature_names)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["inst = X_test.values[0]"]},{"cell_type":"markdown","metadata":{"id":"t0jjBdIjr7UB"},"source":["Then we can explain each data point locally\n","We can see each explanation from the \"perspective\" of the class that we want to focus on"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FxF6XYFmsAlU"},"outputs":[],"source":["exp = explainer.explain_instance(inst, bb.predict_proba)\n","exp.show_in_notebook(show_table=True, show_all=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sp-mm9gOsMCm"},"outputs":[],"source":["exp = explainer.explain_instance(inst, bb.predict_proba, labels=[0])\n","exp.show_in_notebook(show_table=True, show_all=False)"]},{"cell_type":"markdown","metadata":{"id":"BIaCf2ufsruD"},"source":["We can also \"cut\" our explanation and modify the visualization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ty_BN0fosgNU"},"outputs":[],"source":["exp = explainer.explain_instance(inst, bb.predict_proba, num_features=5)\n","exp.show_in_notebook(show_table=True, show_all=False)"]},{"cell_type":"markdown","metadata":{},"source":["You can also manage other options:\n","\n","- top_labels: if not None, ignore labels and produce explanations for\n","                the K labels with highest prediction probabilities, where K is\n","                this parameter.\n","- num_features: maximum number of features present in explanation\n","            num_samples: size of the neighborhood to learn the linear model\n","- distance_metric: the distance metric to use for weights.\n","- model_regressor: sklearn regressor to use in explanation. Defaults\n","                to Ridge regression in LimeBase. Must have model_regressor.coef_\n","                and 'sample_weight' as a parameter to model_regressor.fit()\n","- sampling_method: Method to sample synthetic data. Defaults to Gaussian\n","                sampling. Can also use Latin Hypercube Sampling."]},{"cell_type":"markdown","metadata":{"id":"LezOFFfBtDOQ"},"source":["We can provide the explanation as a pyplot figure"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IGg6IaN3szQR"},"outputs":[],"source":["exp.as_pyplot_figure(label=0)"]},{"cell_type":"markdown","metadata":{"id":"tLfvdh7vtUe5"},"source":["Or manipulate the values as a list"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u2zYa79qtIjF"},"outputs":[],"source":["exp.as_list(0)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["exp.as_map()"]},{"cell_type":"markdown","metadata":{"id":"p43cFtQbAU_r"},"source":["# Lore"]},{"cell_type":"markdown","metadata":{"id":"_2xEa4gg2lsB"},"source":["Lore is an algorithm developed internally to the KDDLab. To use it we will use the xai lib that we imported at the beginning. Note that shap and lime are wrappend in the xai lib, so if you want you can replicate the previous part using the xai lib."]},{"cell_type":"markdown","metadata":{"id":"4NFxGa7Xin3w"},"source":["We then use the `sklearn_classifier_wrapper` from xai lib to wrap our black box and make it ready to explain"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qEyTz9pWijAZ"},"outputs":[],"source":["bbox = sklearn_classifier_wrapper(bb) "]},{"cell_type":"markdown","metadata":{"id":"hdKgHFvCY-PK"},"source":["Now e could also use the imlpementations of SHAP and Lime that are present in the XAI-lib"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"585-As_SZFRy"},"outputs":[],"source":["explainer = LoreTabularExplainer(bbox)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VSXnMvdnA7ZE"},"outputs":[],"source":["inst = X_train.iloc[147].values"]},{"cell_type":"markdown","metadata":{"id":"oae8tvGGBIfB"},"source":["Lore uses various type of neighborhood generation to provide a local explanation based on factuals and counterfactuals"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"exxQ-XNEAxM3"},"outputs":[],"source":["explainer = LoreTabularExplainer(bbox)\n","config = {'neigh_type':'rndgen', 'size':1000, 'ocr':0.1, 'ngen':10}\n","explainer.fit(df, target, config)\n","exp = explainer.explain(inst)\n","print(exp)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JDid8U3BAzvW"},"outputs":[],"source":["print('Instance ',inst)\n","print('True class ',Y_train.iloc[147])\n","print('Predicted class ',bb.predict(inst.reshape(1, -1)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"czk8Vx4CBEoF"},"outputs":[],"source":["exp.plotRules()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q0UqG7HkBHHV"},"outputs":[],"source":["exp.plotCounterfactualRules()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPRMN8k+uZBTOtBgMmy7Owd","collapsed_sections":["nVr_HrArqLx3"],"mount_file_id":"1Xp28aBxPungm1ixO8uI2FxCTbXMolfko","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.1"}},"nbformat":4,"nbformat_minor":0}
